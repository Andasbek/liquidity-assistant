# --- LLM CONFIG ---
LLM_PROVIDER=ollama            # ollama | openai | (пусто = без LLM)
LLM_MODEL=llama3.1             # для ollama: например llama3.1 / mistral / qwen2
OLLAMA_BASE_URL=http://host.docker.internal:11434

# Для OpenAI-совместимого API (vLLM/иное):
# LLM_PROVIDER=openai
# OPENAI_BASE_URL=http://your-vllm-host:8001/v1
# OPENAI_API_KEY=your_token
